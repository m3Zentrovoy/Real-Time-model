{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecf63178",
   "metadata": {},
   "source": [
    "## How to use (3 steps)\n",
    "1) Step 0: run installs once (or skip if already installed).\n",
    "2) Step 1: pick audio from dropdown and click *Load file*.\n",
    "3) Step 2: edit ground truth (GT_PRESETS) and auto-align toggles.\n",
    "4) Step 3: run analysis to see the plot.\n",
    "5) Optional: transcript cell at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c68f4740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa==0.10.1 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (0.10.1)\n",
      "Requirement already satisfied: numpy in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (2.3.5)\n",
      "Requirement already satisfied: pandas in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (3.10.8)\n",
      "Requirement already satisfied: seaborn in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: plotly in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (6.5.0)\n",
      "Requirement already satisfied: scipy in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (1.16.3)\n",
      "Requirement already satisfied: ipywidgets in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (8.1.8)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from librosa==0.10.1) (3.1.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from librosa==0.10.1) (1.8.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from librosa==0.10.1) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from librosa==0.10.1) (5.2.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from librosa==0.10.1) (0.63.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from librosa==0.10.1) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from librosa==0.10.1) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from librosa==0.10.1) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from librosa==0.10.1) (4.15.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from librosa==0.10.1) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from librosa==0.10.1) (1.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from plotly) (2.13.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from ipywidgets) (9.8.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: llvmlite<0.47,>=0.46.0dev0 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from numba>=0.51.0->librosa==0.10.1) (0.46.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from pooch>=1.0->librosa==0.10.1) (4.5.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from pooch>=1.0->librosa==0.10.1) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.10.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.10.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.10.1) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.10.1) (2025.11.12)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from scikit-learn>=0.20.0->librosa==0.10.1) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from soundfile>=0.12.1->librosa==0.10.1) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa==0.10.1) (2.23)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: sentencepiece in /Users/zentrovoy/.venvs/emotion/lib/python3.12/site-packages (0.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa==0.10.1 numpy pandas matplotlib seaborn plotly scipy ipywidgets\n",
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a666cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Scanning audio folder...\n",
      "‚úÖ –ù–∞–π–¥–µ–Ω–æ 14 WAV —Ñ–∞–π–ª–æ–≤. –í—ã–±–µ—Ä–∏ –∏ –Ω–∞–∂–º–∏ 'Load file'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa2230a5a8a41c0b71dc409fbefd4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Pick audio:', options=('untitled #2.wav', 'good_interview.wav', 'untitled‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1 ‚Äî Select audio and compute baseline (run after installs).\n",
    "# Pick a file in the dropdown and click 'Load file'. Baseline uses first 15s.\n",
    "\n",
    "# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ª–æ–∞–¥–µ—Ä –¥–ª—è WAV/MP3 —Å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π –≤ 16 kHz mono\n",
    "import numpy as _np\n",
    "\n",
    "def load_audio_any(path, target_sr=16000):\n",
    "    try:\n",
    "        audio, sr = librosa.load(path, sr=target_sr, mono=True)\n",
    "        return audio.astype(_np.float32), sr\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {path}: {e}\")\n",
    "        return _np.array([], dtype=_np.float32), target_sr\n",
    "\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import os, glob, warnings\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "\n",
    "def extract_features(audio, sr=16000, hop_length=512):\n",
    "    \"\"\"–ò–∑–≤–ª–µ–∫–∞–µ—Ç RMS, pitch, voiced_ratio –¥–ª—è 3—Å –æ–∫–Ω–∞; —É—Å—Ç–æ–π—á–∏–≤ –∫ —Ç–∏—à–∏–Ω–µ.\"\"\"\n",
    "    default_feats = {\n",
    "        'rms': 0.0,\n",
    "        'rms_std': 0.0,\n",
    "        'pitch_jitter': 0.0,\n",
    "        'voiced_ratio': 0.0,\n",
    "        'pitch_mean': 0.0\n",
    "    }\n",
    "    if audio is None or len(audio) == 0:\n",
    "        return default_feats\n",
    "\n",
    "    audio = np.asarray(audio, dtype=np.float32)\n",
    "    audio = np.nan_to_num(audio, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    if np.allclose(audio, 0):\n",
    "        return default_feats\n",
    "\n",
    "    rms = librosa.feature.rms(y=audio, hop_length=hop_length)[0]\n",
    "\n",
    "    try:\n",
    "        pitches, voiced_flag, voiced_probs = librosa.pyin(\n",
    "            audio, fmin=50, fmax=600, sr=sr, hop_length=hop_length\n",
    "        )\n",
    "    except Exception:\n",
    "        pitches = np.full_like(rms, np.nan)\n",
    "        voiced_probs = np.zeros_like(rms)\n",
    "\n",
    "    voiced_probs_clean = voiced_probs[~np.isnan(voiced_probs)]\n",
    "    voiced_ratio = float(np.mean(voiced_probs_clean)) if voiced_probs_clean.size > 0 else 0.0\n",
    "\n",
    "    valid_pitches = pitches[~np.isnan(pitches)]\n",
    "    pitch_jitter = float(np.std(valid_pitches) / np.mean(valid_pitches) * 100) if valid_pitches.size > 1 and np.mean(valid_pitches) > 0 else 0.0\n",
    "    pitch_mean = float(np.nanmean(pitches)) if np.isfinite(np.nanmean(pitches)) else 0.0\n",
    "\n",
    "    return {\n",
    "        'rms': float(np.mean(rms)),\n",
    "        'rms_std': float(np.std(rms)),\n",
    "        'pitch_jitter': pitch_jitter,\n",
    "        'voiced_ratio': voiced_ratio,\n",
    "        'pitch_mean': pitch_mean\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_baseline(audio, sr=16000, window_sec=15):\n",
    "    \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π baseline –Ω–∞ –ø–µ—Ä–≤—ã—Ö window_sec —Å–µ–∫; —É—Å—Ç–æ–π—á–∏–≤ –∫ –∫–æ—Ä–æ—Ç–∫–∏–º —Ñ–∞–π–ª–∞–º.\"\"\"\n",
    "    if audio is None:\n",
    "        return {'rms': 1e-6, 'rms_std': 1e-9, 'pitch_jitter': 1e-3, 'voiced_ratio': 0.0, 'pitch_mean': 0.0}\n",
    "\n",
    "    samples_per_window = max(int(window_sec * sr), 1)\n",
    "    baseline_features = []\n",
    "\n",
    "    for i in range(0, len(audio), samples_per_window):\n",
    "        window = audio[i:i+samples_per_window]\n",
    "        if len(window) >= int(0.5 * sr):\n",
    "            feats = extract_features(window, sr)\n",
    "            baseline_features.append(feats)\n",
    "\n",
    "    if not baseline_features:\n",
    "        baseline_features.append(extract_features(audio, sr))\n",
    "\n",
    "    keys = baseline_features[0].keys()\n",
    "    baseline = {}\n",
    "    for k in keys:\n",
    "        values = [f.get(k, 0.0) for f in baseline_features]\n",
    "        baseline[k] = float(np.nan_to_num(np.mean(values)))\n",
    "\n",
    "    baseline['rms'] = max(baseline.get('rms', 0.0), 1e-6)\n",
    "    baseline['rms_std'] = max(baseline.get('rms_std', 0.0), 1e-9)\n",
    "    baseline['pitch_jitter'] = max(baseline.get('pitch_jitter', 0.0), 1e-3)\n",
    "    return baseline\n",
    "\n",
    "\n",
    "def compute_agitation_score(features, baseline, prev_score=None, smoothing_alpha=0.5, spike_threshold=20.0, max_step=9.0):\n",
    "    \"\"\"–°—á–∏—Ç–∞–µ—Ç agitation 0-100 —Å —É—Å–∏–ª–µ–Ω–Ω—ã–º —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ–º –∏ –∞–Ω—Ç–∏-—Å–ø–∞–π–∫ —Ñ–∏–ª—å—Ç—Ä–æ–º.\n",
    "    - smoothing_alpha (0.4-0.6): –¥–æ–ª—è –Ω–æ–≤–æ–≥–æ –æ–∫–Ω–∞; –æ—Å—Ç–∞–ª—å–Ω–æ–µ - –ø—Ä–æ—à–ª—ã–π score.\n",
    "    - spike_threshold: –ø–æ—Ä–æ–≥ |raw - prev|, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–º –ø—Ä–∏—Ä–æ—Å—Ç —Ä–µ–∂–µ—Ç—Å—è –¥–æ max_step.\n",
    "    - max_step: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –¥–æ–ø—É—Å—Ç–∏–º—ã–π —à–∞–≥ –∑–∞ –æ–∫–Ω–æ –ø—Ä–∏ —Ä–µ–∑–∫–∏—Ö —Å–∫–∞—á–∫–∞—Ö.\n",
    "    \"\"\"\n",
    "    baseline_rms = max(baseline.get('rms', 1e-6) or 1e-6, 1e-6)\n",
    "    rms_ratio = max(float(features.get('rms', 0.0)) / baseline_rms, 0.0)\n",
    "    rms_volatility = max(float(features.get('rms_std', 0.0)) / baseline_rms, 0.0)\n",
    "    pitch_jitter = float(np.nan_to_num(features.get('pitch_jitter', 0.0), nan=0.0, posinf=0.0, neginf=0.0))\n",
    "    voiced_ratio = float(np.nan_to_num(features.get('voiced_ratio', 0.0), nan=0.0, posinf=0.0, neginf=0.0))\n",
    "\n",
    "    rms_term = np.clip((rms_ratio - 0.9), 0, 2.0) * 8.0\n",
    "    jitter_term = np.clip(pitch_jitter, 0, 40) * 1.3\n",
    "    volatility_term = np.clip(rms_volatility, 0, 3.0) * 18.0\n",
    "    voiced_term = np.clip(voiced_ratio, 0, 1.0) * 5.0\n",
    "\n",
    "    raw_score = float(np.clip(rms_term + jitter_term + volatility_term + voiced_term, 0, 100))\n",
    "\n",
    "    if prev_score is None or not np.isfinite(prev_score):\n",
    "        prev_score = raw_score\n",
    "\n",
    "    delta_raw = raw_score - prev_score\n",
    "    direction = np.sign(delta_raw)\n",
    "\n",
    "    if abs(delta_raw) > spike_threshold:\n",
    "        candidate = prev_score + direction * max_step\n",
    "    else:\n",
    "        candidate = smoothing_alpha * raw_score + (1 - smoothing_alpha) * prev_score\n",
    "\n",
    "    candidate = float(np.clip(candidate, 0, 100))\n",
    "    return round(candidate, 1)\n",
    "\n",
    "\n",
    "def get_mood_state(features, baseline, agitation_score, prev_state='CALM', prev_score=None, recent_scores=None, state_streak=1, pending_state=None, pending_count=0):\n",
    "    \"\"\"–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç CALM/TENSE/ESCALATING –ø–æ —Å–≥–ª–∞–∂–µ–Ω–Ω–æ–º—É score —Å –≥–∏—Å—Ç–µ—Ä–µ–∑–∏—Å–æ–º –∏ streak-–ø–æ—Ä–æ–≥–∞–º–∏.\n",
    "    - CALM<->TENSE: —Å–º–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ–¥—Ä—è–¥ >=3 –æ–∫–Ω–∞.\n",
    "    - TENSE<->ESCALATING: —Å–º–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø–æ–¥—Ä—è–¥ >=4 –æ–∫–æ–Ω; –≤—ã—Ö–æ–¥ –∏–∑ ESC –ø—Ä–∏ score <60 —É–¥–µ—Ä–∂–∏–≤–∞–µ–º —á–µ—Ä–µ–∑ streak.\n",
    "    - ESC –∞–∫—Ç–∏–≤–∏—Ä—É–µ–º –ø—Ä–∏ score >=65 –∏–ª–∏ —Ä–æ—Å—Ç–µ >25 –∑–∞ ~3 –æ–∫–Ω–∞ (6-9 c), —É–¥–µ—Ä–∂–∏–≤–∞–µ–º –ø—Ä–∏ score >=60.\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Å—á–µ—Ç—á–∏–∫–∏ streak/pending, –ø–ª—é—Å –º–µ—Ç—Ä–∏–∫–∏.\n",
    "    \"\"\"\n",
    "    recent_scores = recent_scores or []\n",
    "\n",
    "    baseline_rms = max(baseline.get('rms', 1e-6) or 1e-6, 1e-6)\n",
    "    rms_ratio = float(np.clip((features.get('rms', 0.0) / baseline_rms), 0, 10))\n",
    "    pitch_jitter = float(np.nan_to_num(features.get('pitch_jitter', 0.0), nan=0.0, posinf=0.0, neginf=0.0))\n",
    "\n",
    "    last_ref = prev_score if prev_score is not None else (recent_scores[-1] if recent_scores else None)\n",
    "    delta = abs(agitation_score - last_ref) if last_ref is not None else 0.0\n",
    "\n",
    "    if len(recent_scores) >= 3:\n",
    "        growth_current = agitation_score - recent_scores[-3]\n",
    "    else:\n",
    "        growth_current = agitation_score - last_ref if last_ref is not None else 0.0\n",
    "\n",
    "    escalate_cond = (agitation_score >= 65) or (growth_current > 25)\n",
    "    calm_cond = (agitation_score < 30) and (delta < 10)\n",
    "    hold_escalating = (prev_state == 'ESCALATING' and agitation_score >= 60)\n",
    "\n",
    "    if hold_escalating:\n",
    "        candidate_state = 'ESCALATING'\n",
    "    elif escalate_cond:\n",
    "        candidate_state = 'ESCALATING'\n",
    "    elif calm_cond:\n",
    "        candidate_state = 'CALM'\n",
    "    elif agitation_score >= 30:\n",
    "        candidate_state = 'TENSE'\n",
    "    else:\n",
    "        candidate_state = 'CALM'\n",
    "\n",
    "    borderline = False\n",
    "    new_pending_state = pending_state\n",
    "    new_pending_count = pending_count\n",
    "    new_state_streak = state_streak\n",
    "\n",
    "    def required_streak(prev_state, cand_state):\n",
    "        if {'CALM', 'TENSE'} == {prev_state, cand_state}:\n",
    "            return 3\n",
    "        if 'ESCALATING' in (prev_state, cand_state):\n",
    "            return 4\n",
    "        return 3\n",
    "\n",
    "    if candidate_state == prev_state:\n",
    "        final_state = prev_state\n",
    "        new_state_streak = state_streak + 1\n",
    "        new_pending_state = None\n",
    "        new_pending_count = 0\n",
    "    else:\n",
    "        needed = required_streak(prev_state, candidate_state)\n",
    "        if candidate_state == pending_state:\n",
    "            new_pending_count = pending_count + 1\n",
    "        else:\n",
    "            new_pending_state = candidate_state\n",
    "            new_pending_count = 1\n",
    "\n",
    "        if new_pending_count >= needed:\n",
    "            final_state = candidate_state\n",
    "            new_state_streak = 1\n",
    "            new_pending_state = None\n",
    "            new_pending_count = 0\n",
    "        else:\n",
    "            final_state = prev_state\n",
    "            new_state_streak = state_streak + 1\n",
    "            borderline = True\n",
    "\n",
    "    return {\n",
    "        'state': final_state,\n",
    "        'agitation_score': round(float(agitation_score), 1),\n",
    "        'rms_ratio': round(rms_ratio, 2),\n",
    "        'pitch_jitter': round(pitch_jitter, 1),\n",
    "        'state_streak': new_state_streak,\n",
    "        'pending_state': new_pending_state,\n",
    "        'pending_count': new_pending_count,\n",
    "        'borderline_state': borderline\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"üìÇ Scanning audio folder...\")\n",
    "audio_dir = str((Path.cwd() / 'audio_samples').resolve())\n",
    "wav_files = glob.glob(os.path.join(audio_dir, '*.wav')) + glob.glob(os.path.join(audio_dir, '*.mp3'))\n",
    "\n",
    "audio = None\n",
    "baseline = None\n",
    "duration = 0\n",
    "sr = 16000\n",
    "audio_ready = False\n",
    "selected_path = None\n",
    "\n",
    "if not wav_files:\n",
    "    dropdown = None\n",
    "    print(\"‚ùå No audio files found. Check path.\")\n",
    "else:\n",
    "    file_names = [os.path.basename(f) for f in wav_files]\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=file_names,\n",
    "        description='Pick audio:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    load_btn = widgets.Button(description='Load file', button_style='primary')\n",
    "    status = widgets.Output()\n",
    "\n",
    "    def load_selected(_):\n",
    "        global audio, baseline, duration, sr, audio_ready, selected_path\n",
    "        with status:\n",
    "            status.clear_output()\n",
    "            if dropdown.value is None:\n",
    "                print(\"‚ö†Ô∏è Pick a file in the dropdown.\")\n",
    "                audio_ready = False\n",
    "                return\n",
    "            selected_file = os.path.join(audio_dir, dropdown.value)\n",
    "            if not os.path.exists(selected_file):\n",
    "                print(\"‚ùå File not found (maybe removed).\")\n",
    "                audio_ready = False\n",
    "                return\n",
    "            print(f\"üîÑ Loading: {dropdown.value}\")\n",
    "            audio, sr = load_audio_any(selected_file, target_sr=16000)\n",
    "            duration = len(audio) / sr if sr else 0\n",
    "            baseline_audio = audio[:min(15 * sr, len(audio))]\n",
    "            baseline = compute_baseline(baseline_audio, sr)\n",
    "            readable = {k: round(v, 3) for k, v in baseline.items()}\n",
    "            selected_path = selected_file\n",
    "            audio_ready = True\n",
    "            print(f\"‚úÖ Loaded: {duration:.1f} —Å–µ–∫, sr={sr}Hz\")\n",
    "            print(\"‚úÖ Baseline:\", readable)\n",
    "\n",
    "    load_btn.on_click(load_selected)\n",
    "\n",
    "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(wav_files)} WAV —Ñ–∞–π–ª–æ–≤. –í—ã–±–µ—Ä–∏ –∏ –Ω–∞–∂–º–∏ 'Load file'.\")\n",
    "    display(widgets.VBox([dropdown, load_btn, status]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1215368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ GT for 'test_emotions': [{'start': 0, 'end': 9, 'label': 'CALM'}, {'start': 9, 'end': 21, 'label': 'TENSE'}, {'start': 21, 'end': 37, 'label': 'ESCALATION'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2 ‚Äî Ground truth setup (edit presets or manual list)\n",
    "from pathlib import Path\n",
    "\n",
    "# Keys are audio file stems in lowercase with underscores\n",
    "GT_PRESETS = {\n",
    "    \"test_emotions\": [\n",
    "        {\"start\": 0, \"end\": 9, \"label\": \"CALM\"},\n",
    "        {\"start\": 9, \"end\": 21, \"label\": \"TENSE\"},\n",
    "        {\"start\": 21, \"end\": 37, \"label\": \"ESCALATION\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Optional auto-alignment of GT to the model signal\n",
    "AUTO_ALIGN_GT = True\n",
    "LAG_SEARCH_SEC = (-5, 5)  # search window in seconds\n",
    "LAG_STEPS = 41            # number of lag samples (e.g., 41 -> 0.25s step for +-5s)\n",
    "\n",
    "audio_key = Path(selected_path).stem.lower().replace(\" \", \"_\") if 'selected_path' in globals() and selected_path else None\n",
    "manual_annotations = GT_PRESETS.get(audio_key, []).copy() if audio_key else []\n",
    "\n",
    "if manual_annotations:\n",
    "    print(f\"üéØ GT for '{audio_key}': {manual_annotations}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No preset GT found. Edit GT_PRESETS or set manual_annotations manually.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "729f6b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2b ‚Äî Optional text/ASR pipelines (leave as-is if no network)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import pipeline\n",
    "except Exception:\n",
    "    pipeline = None\n",
    "    print(\"‚ö†Ô∏è transformers –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —Å–ª–æ–π –±—É–¥–µ—Ç –æ—Ç–∫–ª—é—á–µ–Ω\")\n",
    "\n",
    "asr_pipe = None\n",
    "sent_pipe = None\n",
    "\n",
    "def load_text_pipelines(asr_model=\"openai/whisper-small\", cls_model=\"nlptown/bert-base-multilingual-uncased-sentiment\"): \n",
    "    \"\"\"Lazy load ASR –∏ text model; –±–µ–∑–æ–ø–∞—Å–Ω–æ –ø–∞–¥–∞–µ—Ç, –µ—Å–ª–∏ –Ω–µ—Ç —Å–µ—Ç–∏/–º–æ–¥–µ–ª–µ–π.\"\"\"\n",
    "    global asr_pipe, sent_pipe\n",
    "    if pipeline is None:\n",
    "        print(\"‚ö†Ô∏è transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\")\n",
    "        return False\n",
    "    device = 0 if \"torch\" in globals() and hasattr(torch, \"cuda\") and torch.cuda.is_available() else -1\n",
    "    if asr_pipe is None:\n",
    "        try:\n",
    "            asr_pipe = pipeline(\"automatic-speech-recognition\", model=asr_model, device=device)\n",
    "        except Exception as e:\n",
    "            print(\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å ASR:\", e)\n",
    "            asr_pipe = None\n",
    "    if sent_pipe is None:\n",
    "        try:\n",
    "            sent_pipe = pipeline(\"text-classification\", model=cls_model, device=device)\n",
    "        except Exception as e:\n",
    "            print(\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å:\", e)\n",
    "            sent_pipe = None\n",
    "    return asr_pipe is not None and sent_pipe is not None\n",
    "\n",
    "\n",
    "def transcribe_and_classify(path, window_sec=5.0, hop_sec=5.0, sr=16000):\n",
    "    \"\"\"Split audio into windows, run ASR + sentiment; returns a DataFrame with time and sentiment_score.\"\"\"\n",
    "    if asr_pipe is None or sent_pipe is None:\n",
    "        return pd.DataFrame()\n",
    "    audio, _ = librosa.load(path, sr=sr)\n",
    "    win = int(window_sec * sr)\n",
    "    hop = int(hop_sec * sr)\n",
    "    rows = []\n",
    "    for start in range(0, len(audio), hop):\n",
    "        end = min(len(audio), start + win)\n",
    "        chunk = audio[start:end]\n",
    "        if len(chunk) < 0.5 * sr:\n",
    "            continue\n",
    "        try:\n",
    "            text = asr_pipe({\"array\": chunk, \"sampling_rate\": sr}).get(\"text\", \"\")\n",
    "        except Exception:\n",
    "            text = \"\"\n",
    "        try:\n",
    "            senti = sent_pipe(text)[0]\n",
    "            sentiment_score = float(senti.get(\"score\", 0.0)) if isinstance(senti, dict) else 0.0\n",
    "        except Exception:\n",
    "            sentiment_score = 0.0\n",
    "        rows.append({\n",
    "            \"start_sec\": start / sr,\n",
    "            \"end_sec\": end / sr,\n",
    "            \"sentiment_score\": sentiment_score,\n",
    "            \"text\": text\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def fuse_audio_text(df_audio: pd.DataFrame, df_text: pd.DataFrame):\n",
    "    \"\"\"Late fusion: audio (fast) + text (calibration). Returns df with fusion_score.\"\"\"\n",
    "    if df_audio is None or df_audio.empty:\n",
    "        return df_audio\n",
    "    df_audio = df_audio.copy()\n",
    "    if df_text is None or df_text.empty:\n",
    "        df_audio[\"fusion_score\"] = df_audio.get(\"vis_agitation\", df_audio.get(\"agitation_score\", 0.0))\n",
    "        df_audio[\"sentiment_score\"] = 0.0\n",
    "        return df_audio\n",
    "\n",
    "    df_text = df_text.copy()\n",
    "    df_text[\"mid_sec\"] = (df_text[\"start_sec\"] + df_text[\"end_sec\"]) / 2\n",
    "    if \"time_sec\" in df_audio.columns:\n",
    "        df_audio[\"mid_sec\"] = df_audio[\"time_sec\"]\n",
    "    elif \"start_sec\" in df_audio.columns and \"end_sec\" in df_audio.columns:\n",
    "        df_audio[\"mid_sec\"] = (df_audio[\"start_sec\"] + df_audio[\"end_sec\"]) / 2\n",
    "    else:\n",
    "        df_audio[\"mid_sec\"] = range(len(df_audio))\n",
    "\n",
    "    merged = pd.merge_asof(\n",
    "        df_audio.sort_values(\"mid_sec\"),\n",
    "        df_text.sort_values(\"mid_sec\")[[\"mid_sec\", \"sentiment_score\"]],\n",
    "        on=\"mid_sec\", direction=\"nearest\", tolerance=3\n",
    "    )\n",
    "    merged[\"sentiment_score\"] = merged[\"sentiment_score\"].fillna(0.0)\n",
    "\n",
    "    base = merged.get(\"vis_agitation\", merged.get(\"agitation_score\", merged.get(\"frustration_proxy\", 0.0)))\n",
    "    text_component = merged[\"sentiment_score\"]\n",
    "    if text_component.max() <= 1:\n",
    "        text_component = text_component * 100\n",
    "    \n",
    "    # –°–≥–ª–∞–∂–∏–≤–∞–µ–º –∏ —Å—Ç–∞–≤–∏–º –ø–æ—Ä–æ–≥, —á—Ç–æ–±—ã –∏–∑–±–µ–≥–∞—Ç—å –∫—Ä–∞—Ç–∫–∏—Ö –ø—Ä–æ–≤–∞–ª–æ–≤\n",
    "    text_component = text_component.rolling(window=3, min_periods=1, center=True).mean()\n",
    "    text_component = text_component.clip(lower=20)  # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤–∫–ª–∞–¥ —Ç–µ–∫—Å—Ç–∞\n",
    "    merged[\"fusion_score\"] = 0.7 * base + 0.3 * text_component\n",
    "            \n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c1b2924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Analyzing 3s windows...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e760ab1fcb4bcdaedbb66d5bc581d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, bar_style='info', description='‚è≥', max=37), HTML(value='‚è≥ Preparing...')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Analysis finished! 37 –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–æ—á–µ–∫\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>agitation_score</th>\n",
       "      <th>rms_ratio</th>\n",
       "      <th>pitch_jitter</th>\n",
       "      <th>time_sec</th>\n",
       "      <th>tension_trend</th>\n",
       "      <th>dialogue_escalation</th>\n",
       "      <th>rolling_mean_15s</th>\n",
       "      <th>rolling_mean_30s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CALM</td>\n",
       "      <td>33.200</td>\n",
       "      <td>1.050</td>\n",
       "      <td>14.300</td>\n",
       "      <td>1.500</td>\n",
       "      <td>33.200</td>\n",
       "      <td>False</td>\n",
       "      <td>33.200</td>\n",
       "      <td>33.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CALM</td>\n",
       "      <td>36.600</td>\n",
       "      <td>1.090</td>\n",
       "      <td>18.200</td>\n",
       "      <td>2.500</td>\n",
       "      <td>33.300</td>\n",
       "      <td>False</td>\n",
       "      <td>34.900</td>\n",
       "      <td>34.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TENSE</td>\n",
       "      <td>34.400</td>\n",
       "      <td>1.090</td>\n",
       "      <td>14.300</td>\n",
       "      <td>3.500</td>\n",
       "      <td>33.300</td>\n",
       "      <td>False</td>\n",
       "      <td>34.733</td>\n",
       "      <td>34.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TENSE</td>\n",
       "      <td>32.700</td>\n",
       "      <td>0.990</td>\n",
       "      <td>12.900</td>\n",
       "      <td>4.500</td>\n",
       "      <td>33.300</td>\n",
       "      <td>False</td>\n",
       "      <td>34.225</td>\n",
       "      <td>34.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TENSE</td>\n",
       "      <td>32.400</td>\n",
       "      <td>1.020</td>\n",
       "      <td>13.500</td>\n",
       "      <td>5.500</td>\n",
       "      <td>33.300</td>\n",
       "      <td>False</td>\n",
       "      <td>33.860</td>\n",
       "      <td>33.860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   state  agitation_score  rms_ratio  pitch_jitter  time_sec  tension_trend  \\\n",
       "0   CALM           33.200      1.050        14.300     1.500         33.200   \n",
       "1   CALM           36.600      1.090        18.200     2.500         33.300   \n",
       "2  TENSE           34.400      1.090        14.300     3.500         33.300   \n",
       "3  TENSE           32.700      0.990        12.900     4.500         33.300   \n",
       "4  TENSE           32.400      1.020        13.500     5.500         33.300   \n",
       "\n",
       "   dialogue_escalation  rolling_mean_15s  rolling_mean_30s  \n",
       "0                False            33.200            33.200  \n",
       "1                False            34.900            34.900  \n",
       "2                False            34.733            34.733  \n",
       "3                False            34.225            34.225  \n",
       "4                False            33.860            33.860  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Auto-align GT: shift -1.25s (corr=0.514)\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "skip",
         "line": {
          "color": "rgba(120,120,120,0.3)",
          "width": 1
         },
         "mode": "lines",
         "name": "Acoustic (raw)",
         "type": "scatter",
         "x": {
          "bdata": "AAAAAAAA+D8AAAAAAAAEQAAAAAAAAAxAAAAAAAAAEkAAAAAAAAAWQAAAAAAAABpAAAAAAAAAHkAAAAAAAAAhQAAAAAAAACNAAAAAAAAAJUAAAAAAAAAnQAAAAAAAAClAAAAAAAAAK0AAAAAAAAAtQAAAAAAAAC9AAAAAAACAMEAAAAAAAIAxQAAAAAAAgDJAAAAAAACAM0AAAAAAAIA0QAAAAAAAgDVAAAAAAACANkAAAAAAAIA3QAAAAAAAgDhAAAAAAACAOUAAAAAAAIA6QAAAAAAAgDtAAAAAAACAPEAAAAAAAIA9QAAAAAAAgD5AAAAAAACAP0AAAAAAAEBAQAAAAAAAwEBAAAAAAABAQUAAAAAAAMBBQAAAAAAAQEJA7FG4HoWrQkA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "mpmZmZmZQEDNzMzMzExCQDMzMzMzM0FAmpmZmZlZQEAzMzMzMzNAQM3MzMzMzD9AMzMzMzMzOUCamZmZmRlBQJqZmZmZGUFAmpmZmZkZQUCamZmZmRlBQJqZmZmZWUFAzczMzMzMQUAAAAAAAABEQM3MzMzMjEVAzczMzMxMRkAzMzMzM/NDQAAAAAAAAEJAzczMzMwMQUCamZmZmdlBQDMzMzMzM0VAMzMzMzPzSUAzMzMzMzNLQDMzMzMzs0hAMzMzMzOzRUBmZmZmZqZDQJqZmZmZWUFAMzMzMzMzQUBmZmZmZmZBQDMzMzMzc0JAAAAAAABAQ0DNzMzMzAxCQAAAAAAAQEFAMzMzMzOzP0CamZmZmZk9QM3MzMzMTDtAmpmZmZmZPEA=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "line": {
          "color": "#E67E22",
          "width": 3
         },
         "mode": "lines",
         "name": "Acoustic (smoothed)",
         "type": "scatter",
         "x": {
          "bdata": "AAAAAAAA+D8AAAAAAAAEQAAAAAAAAAxAAAAAAAAAEkAAAAAAAAAWQAAAAAAAABpAAAAAAAAAHkAAAAAAAAAhQAAAAAAAACNAAAAAAAAAJUAAAAAAAAAnQAAAAAAAAClAAAAAAAAAK0AAAAAAAAAtQAAAAAAAAC9AAAAAAACAMEAAAAAAAIAxQAAAAAAAgDJAAAAAAACAM0AAAAAAAIA0QAAAAAAAgDVAAAAAAACANkAAAAAAAIA3QAAAAAAAgDhAAAAAAACAOUAAAAAAAIA6QAAAAAAAgDtAAAAAAACAPEAAAAAAAIA9QAAAAAAAgD5AAAAAAACAP0AAAAAAAEBAQAAAAAAAwEBAAAAAAABAQUAAAAAAAMBBQAAAAAAAQEJA7FG4HoWrQkA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "mpmZmZmZQEA0MzMzM3NBQN7d3d3dXUFAzczMzMwcQUCuR+F6FO5AQCIiIiIiwkBAoQ7qoA4qQEAAAAAAAEhAQPRJn/RJX0BAUrgehetxQEBBnhLkKYFAQDMzMzMzk0BAtlIrtVKrQECEOqiDOuhAQHd3d3d3N0FAWvKLJb+YQUC1gU4b6LRBQMP1KFyPwkFATxvotIHOQUCrqqqqqupBQIXrUbgeRUJAKlyPwvUoQ0BVVVVVVdVDQNajcD0KV0RAJr9Y8oulREAREREREdFEQBERERER0URABzptoNPGREDbQKcNdJpEQCW/WPKLZURAseQXS34xREARERERERFEQEREREREBERAMJb8YsnvQ0C8u7u7u7tDQHsUrkfhOkNAdNpApw10QkA=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "line": {
          "color": "#C0392B",
          "dash": "dash",
          "width": 2.5
         },
         "mode": "lines",
         "name": "Fusion (audio + text)",
         "type": "scatter",
         "x": {
          "bdata": "AAAAAAAA+D8AAAAAAAAEQAAAAAAAAAxAAAAAAAAAEkAAAAAAAAAWQAAAAAAAABpAAAAAAAAAHkAAAAAAAAAhQAAAAAAAACNAAAAAAAAAJUAAAAAAAAAnQAAAAAAAAClAAAAAAAAAK0AAAAAAAAAtQAAAAAAAAC9AAAAAAACAMEAAAAAAAIAxQAAAAAAAgDJAAAAAAACAM0AAAAAAAIA0QAAAAAAAgDVAAAAAAACANkAAAAAAAIA3QAAAAAAAgDhAAAAAAACAOUAAAAAAAIA6QAAAAAAAgDtAAAAAAACAPEAAAAAAAIA9QAAAAAAAgD5AAAAAAACAP0AAAAAAAEBAQAAAAAAAwEBAAAAAAABAQUAAAAAAAMBBQAAAAAAAQEJA7FG4HoWrQkA=",
          "dtype": "f8"
         },
         "y": {
          "bdata": "H4VrnijEQkDYo/CJelxDQOi0AZuLTUNAwvUofTRBQ0Bg5VAYtUFDQEt+sa4mRENAcD0KobLZQkCamZljqO5CQCoaiasnUEJAbOf7RmSuQUAu1VnZQQpBQArXI/fhFkFAf1Me0sQnQUD2KNwnTB9BQGygU7GoI0FA2FzIVqY0QUCYJ2qcXEhBQDzfz0nrUUFA6t8IP05/QUCqqiqiCrhBQESLbPxhHEJAt/P96d67QkAiIiI3iDRDQK9yaJQ8vERAmlJx5xkgRkDyiyWeeWtHQPKLJZ55a0dAOEIZnE5kR0BmrQ6pfphGQM2FpIm1xkVAr7kQVYX1RECMJT950t5EQGLJr7bc1URA7jWXeICpQ0DQaYNJCmdCQLx0E4vR7kBAhDL6zaNjQEA=",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "color": "#2c3e50",
           "size": 12
          },
          "showarrow": false,
          "text": "CALM",
          "x": 3.25,
          "xref": "x",
          "y": 0.99,
          "yref": "paper"
         },
         {
          "font": {
           "color": "#2c3e50",
           "size": 12
          },
          "showarrow": false,
          "text": "TENSE",
          "x": 13.75,
          "xref": "x",
          "y": 0.99,
          "yref": "paper"
         },
         {
          "font": {
           "color": "#2c3e50",
           "size": 12
          },
          "showarrow": false,
          "text": "ESCALATION",
          "x": 27.75,
          "xref": "x",
          "y": 0.99,
          "yref": "paper"
         }
        ],
        "font": {
         "size": 13
        },
        "height": 560,
        "hovermode": "x unified",
        "legend": {
         "orientation": "h",
         "x": 0,
         "xanchor": "left",
         "y": 1.15,
         "yanchor": "bottom"
        },
        "margin": {
         "b": 50,
         "l": 70,
         "r": 20,
         "t": 90
        },
        "paper_bgcolor": "white",
        "plot_bgcolor": "white",
        "shapes": [
         {
          "fillcolor": "rgba(46, 204, 113, 0.55)",
          "layer": "below",
          "line": {
           "width": 0
          },
          "type": "rect",
          "x0": -1.25,
          "x1": 7.75,
          "y0": 0,
          "y1": 1,
          "yref": "y2"
         },
         {
          "fillcolor": "rgba(243, 156, 18, 0.55)",
          "layer": "below",
          "line": {
           "width": 0
          },
          "type": "rect",
          "x0": 7.75,
          "x1": 19.75,
          "y0": 0,
          "y1": 1,
          "yref": "y2"
         },
         {
          "fillcolor": "rgba(231, 76, 60, 0.55)",
          "layer": "below",
          "line": {
           "width": 0
          },
          "type": "rect",
          "x0": 19.75,
          "x1": 35.75,
          "y0": 0,
          "y1": 1,
          "yref": "y2"
         },
         {
          "fillcolor": "rgba(46,204,113,0.08)",
          "layer": "below",
          "line": {
           "width": 0
          },
          "type": "rect",
          "x0": 0,
          "x1": 1,
          "xref": "x domain",
          "y0": 0,
          "y1": 50,
          "yref": "y"
         },
         {
          "fillcolor": "rgba(231,76,60,0.08)",
          "layer": "below",
          "line": {
           "width": 0
          },
          "type": "rect",
          "x0": 0,
          "x1": 1,
          "xref": "x domain",
          "y0": 50,
          "y1": 100,
          "yref": "y"
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Agitation vs Ground Truth ‚Äî test emotions.wav"
        },
        "xaxis": {
         "title": {
          "text": "Time (sec)"
         }
        },
        "yaxis": {
         "domain": [
          0,
          0.82
         ],
         "range": [
          0,
          100
         ],
         "title": {
          "text": "Agitation score (0‚Äì100)"
         }
        },
        "yaxis2": {
         "domain": [
          0.86,
          1
         ],
         "range": [
          0,
          1
         ],
         "visible": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3 ‚Äî Analyze and visualize (run after GT setup)\n",
    "if 'audio_ready' not in globals() or not audio_ready or audio is None or len(audio) == 0:\n",
    "    print(\"‚ö†Ô∏è No audio to analyze. Load audio in Step 1 first.\")\n",
    "else:\n",
    "    window_sec = 3\n",
    "    hop_sec = 1\n",
    "    sr = 16000\n",
    "    window_samples = int(window_sec * sr)\n",
    "    hop_samples = int(hop_sec * sr)\n",
    "\n",
    "    starts = list(range(0, max(len(audio) - window_samples, 0) + hop_samples, hop_samples))\n",
    "    if not starts:\n",
    "        starts = [0]\n",
    "\n",
    "    print(\"üîÑ Analyzing 3s windows...\")\n",
    "    progress = widgets.IntProgress(value=0, min=0, max=len(starts), description='‚è≥', bar_style='info')\n",
    "    progress_label = widgets.HTML(value=\"‚è≥ Preparing...\")\n",
    "    display(widgets.VBox([progress, progress_label]))\n",
    "\n",
    "    results = []\n",
    "    prev_state = 'CALM'\n",
    "    prev_agitation = None\n",
    "    last_active_agitation = None\n",
    "    last_active_state = 'CALM'\n",
    "    tension_trend = None\n",
    "    score_history = []\n",
    "    trend_alpha = 0.03  # –º–µ–¥–ª–µ–Ω–Ω—ã–π —Ç—Ä–µ–Ω–¥ ~30-50—Å\n",
    "    state_streak = 0\n",
    "    pending_state = None\n",
    "    pending_count = 0\n",
    "\n",
    "    pause_voiced_thr = 0.12  # —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–µ–µ –∫ —Ç–∏—à–∏–Ω–µ\n",
    "    pause_rms_scale = 0.8\n",
    "\n",
    "    for idx, start_sample in enumerate(starts):\n",
    "        end_sample = min(len(audio), start_sample + window_samples)\n",
    "        window_audio = audio[start_sample:end_sample]\n",
    "\n",
    "        if len(window_audio) < int(0.5 * sr) and len(audio) > int(window_samples):\n",
    "            continue\n",
    "\n",
    "        features = extract_features(window_audio, sr)\n",
    "        baseline_rms = max(baseline.get('rms', 1e-6) or 1e-6, 1e-6)\n",
    "        voiced_ratio_val = float(np.nan_to_num(features.get('voiced_ratio', 0.0), nan=0.0, posinf=0.0, neginf=0.0))\n",
    "        rms_val = float(np.nan_to_num(features.get('rms', 0.0), nan=0.0, posinf=0.0, neginf=0.0))\n",
    "        is_pause = (voiced_ratio_val < pause_voiced_thr) and (rms_val < baseline_rms * pause_rms_scale)\n",
    "\n",
    "        if is_pause and last_active_agitation is not None:\n",
    "            agitation_score = last_active_agitation\n",
    "            state_for_row = last_active_state\n",
    "            rms_ratio_val = float(np.clip(rms_val / baseline_rms, 0, 10))\n",
    "            pitch_jitter_val = float(np.nan_to_num(features.get('pitch_jitter', 0.0), nan=0.0, posinf=0.0, neginf=0.0))\n",
    "            if tension_trend is None:\n",
    "                tension_trend = agitation_score\n",
    "            dialogue_escalation = (tension_trend > 50) and (agitation_score > 65)\n",
    "            out_row = {\n",
    "                'state': state_for_row,\n",
    "                'agitation_score': round(float(agitation_score), 1),\n",
    "                'rms_ratio': round(rms_ratio_val, 2),\n",
    "                'pitch_jitter': round(pitch_jitter_val, 1),\n",
    "                'time_sec': round(((start_sample + end_sample) / 2) / sr, 2),\n",
    "                'tension_trend': round(float(tension_trend), 1),\n",
    "                'dialogue_escalation': bool(dialogue_escalation)\n",
    "            }\n",
    "            results.append(out_row)\n",
    "            score_history.append(agitation_score)\n",
    "        else:\n",
    "            agitation_score = compute_agitation_score(\n",
    "                features, baseline, prev_score=prev_agitation,\n",
    "                smoothing_alpha=0.5, spike_threshold=20.0, max_step=9.0\n",
    "            )\n",
    "\n",
    "            if tension_trend is None:\n",
    "                tension_trend = agitation_score\n",
    "            else:\n",
    "                tension_trend = trend_alpha * agitation_score + (1 - trend_alpha) * tension_trend\n",
    "\n",
    "            mood_info = get_mood_state(\n",
    "                features, baseline, agitation_score,\n",
    "                prev_state=prev_state, prev_score=prev_agitation,\n",
    "                recent_scores=score_history,\n",
    "                state_streak=state_streak,\n",
    "                pending_state=pending_state,\n",
    "                pending_count=pending_count\n",
    "            )\n",
    "\n",
    "            dialogue_escalation = (tension_trend > 50) and (agitation_score > 65)\n",
    "\n",
    "            out_row = {\n",
    "                'state': mood_info['state'],\n",
    "                'agitation_score': mood_info['agitation_score'],\n",
    "                'rms_ratio': mood_info['rms_ratio'],\n",
    "                'pitch_jitter': mood_info['pitch_jitter'],\n",
    "                'time_sec': round(((start_sample + end_sample) / 2) / sr, 2),\n",
    "                'tension_trend': round(float(tension_trend), 1),\n",
    "                'dialogue_escalation': bool(dialogue_escalation)\n",
    "            }\n",
    "            results.append(out_row)\n",
    "\n",
    "            score_history.append(agitation_score)\n",
    "            prev_agitation = agitation_score\n",
    "            prev_state = mood_info['state']\n",
    "            state_streak = mood_info.get('state_streak', state_streak)\n",
    "            pending_state = mood_info.get('pending_state', None)\n",
    "            pending_count = mood_info.get('pending_count', 0)\n",
    "            last_active_agitation = agitation_score\n",
    "            last_active_state = mood_info['state']\n",
    "\n",
    "        progress.value = idx + 1\n",
    "        progress_label.value = (\n",
    "            f\"üîé Window {idx+1}/{len(starts)} ‚Äî t={round(((start_sample + end_sample) / 2) / sr, 2):.1f}s | {results[-1]['state']} | \"\n",
    "            f\"score={results[-1]['agitation_score']:.1f} | trend={results[-1]['tension_trend']:.1f}\"\n",
    "        )\n",
    "\n",
    "    if not results:\n",
    "        fallback_features = extract_features(audio, sr)\n",
    "        fallback_agitation = compute_agitation_score(fallback_features, baseline, prev_score=None)\n",
    "        fallback_trend = fallback_agitation\n",
    "        fallback_mood = get_mood_state(fallback_features, baseline, fallback_agitation)\n",
    "        out_row = {\n",
    "            'state': fallback_mood['state'],\n",
    "            'agitation_score': fallback_mood['agitation_score'],\n",
    "            'rms_ratio': fallback_mood['rms_ratio'],\n",
    "            'pitch_jitter': fallback_mood['pitch_jitter'],\n",
    "            'time_sec': round(len(audio) / (2 * sr), 2),\n",
    "            'tension_trend': round(float(fallback_trend), 1),\n",
    "            'dialogue_escalation': bool(False)\n",
    "        }\n",
    "        results.append(out_row)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # –î–æ–ø. —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "    df['rolling_mean_15s'] = df['agitation_score'].rolling(window=15, min_periods=1).mean()\n",
    "    df['rolling_mean_30s'] = df['agitation_score'].rolling(window=30, min_periods=1).mean()\n",
    "\n",
    "    # Session-level stats for agitation\n",
    "    session_mean = float(df['agitation_score'].mean())\n",
    "    session_std = float(df['agitation_score'].std(ddof=0) or 0.0)\n",
    "    session_p90 = float(df['agitation_score'].quantile(0.9))\n",
    "    df.attrs['session_mean_agitation'] = session_mean\n",
    "    df.attrs['session_std_agitation'] = session_std\n",
    "    df.attrs['session_p90_agitation'] = session_p90\n",
    "    df.attrs['session_state'] = 'CALM_SESSION' if (session_p90 < 40 and session_mean < 35) else 'NORMAL_SESSION'\n",
    "\n",
    "    print(f\"‚úÖ Analysis finished! {len(df)} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–æ—á–µ–∫\")\n",
    "    display(df.head())\n",
    "\n",
    "    # –ì–æ—Ç–æ–≤–∏–º –≤–∏–∑—É–∞–ª—å–Ω—ã–π df\n",
    "    def _prepare_agitation_vis(df_in: pd.DataFrame, window_points_30: int = 30):\n",
    "        if df_in.empty or 'agitation_score' not in df_in.columns:\n",
    "            return df_in\n",
    "        dfv = df_in.copy()\n",
    "        mean_fp = dfv.attrs.get('session_mean_agitation', float(dfv['agitation_score'].mean()))\n",
    "        std_fp = dfv.attrs.get('session_std_agitation', float(dfv['agitation_score'].std(ddof=0) or 0.0))\n",
    "        eps = 1e-6\n",
    "        dfv['ag_centered'] = dfv['agitation_score'] - mean_fp\n",
    "        dfv['ag_z'] = dfv['ag_centered'] / max(std_fp, eps)\n",
    "        window = max(3, int(window_points_30))\n",
    "        dfv['ag_centered_30s'] = dfv['ag_centered'].rolling(window=window, min_periods=1).mean()\n",
    "        session_state_loc = dfv.attrs.get('session_state', 'NORMAL_SESSION')\n",
    "        calm_level = 35.0\n",
    "        calm_band = 5.0\n",
    "        if session_state_loc == 'CALM_SESSION':\n",
    "            vis = calm_level + dfv['ag_centered_30s']\n",
    "            vis = vis.clip(calm_level - calm_band, calm_level + calm_band)\n",
    "            dfv['vis_agitation'] = vis\n",
    "        else:\n",
    "            dfv['vis_agitation'] = dfv['rolling_mean_15s']\n",
    "        return dfv\n",
    "\n",
    "    df_vis = _prepare_agitation_vis(df)\n",
    "\n",
    "    # –°–µ–º–∞–Ω—Ç–∏–∫–∞: ASR + —Ç–µ–∫—Å—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "    df_text = pd.DataFrame()\n",
    "    if 'selected_path' in globals() and selected_path and load_text_pipelines():\n",
    "        df_text = transcribe_and_classify(selected_path, window_sec=5.0, hop_sec=5.0, sr=16000)\n",
    "        if df_text.empty:\n",
    "            print('‚ö†Ô∏è Semantic layer unavailable (ASR/classifier not loaded)')\n",
    "    else:\n",
    "        print('‚ÑπÔ∏è –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —Å–ª–æ–π –Ω–µ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω (no selected_path or model)')\n",
    "\n",
    "\n",
    "    df_fused = fuse_audio_text(df_vis, df_text)\n",
    "    \n",
    "    # Ground truth to plot (with optional auto-align)\n",
    "    def _align_gt(manual_annotations, df_fused, score_col, search_range=(-5,5), steps=41):\n",
    "        import numpy as _np\n",
    "        if not manual_annotations or df_fused.empty:\n",
    "            return manual_annotations, 0.0, None\n",
    "        t = df_fused[\"time_sec\"].to_numpy()\n",
    "        sig = df_fused[score_col].to_numpy()\n",
    "        sig = (sig - _np.nanmean(sig)) / (_np.nanstd(sig) + 1e-6)\n",
    "        label_levels = {\"CALM\": 0, \"TENSE\": 1, \"ESCALATION\": 2}\n",
    "        lags = _np.linspace(search_range[0], search_range[1], steps)\n",
    "        best_lag, best_corr = 0.0, -1.0\n",
    "        best_shifted = manual_annotations\n",
    "        for lag in lags:\n",
    "            mask = _np.zeros_like(sig, dtype=float)\n",
    "            for seg in manual_annotations:\n",
    "                x0 = seg[\"start\"] + lag\n",
    "                x1 = seg[\"end\"] + lag\n",
    "                m = (t >= x0) & (t < x1)\n",
    "                mask[m] = label_levels.get(seg[\"label\"].upper(), 0)\n",
    "            if mask.std() == 0:\n",
    "                continue\n",
    "            corr = _np.corrcoef(sig, mask)[0, 1]\n",
    "            if _np.isnan(corr):\n",
    "                continue\n",
    "            if corr > best_corr:\n",
    "                best_corr = corr\n",
    "                best_lag = float(lag)\n",
    "                best_shifted = [\n",
    "                    {\"start\": seg[\"start\"] + lag, \"end\": seg[\"end\"] + lag, \"label\": seg[\"label\"]}\n",
    "                    for seg in manual_annotations\n",
    "                ]\n",
    "        return best_shifted, best_lag, best_corr\n",
    "    \n",
    "    annotations_for_plot = manual_annotations.copy() if 'manual_annotations' in globals() else []\n",
    "    best_lag = 0.0\n",
    "    best_corr = None\n",
    "    score_col = \"fusion_score\" if \"fusion_score\" in df_fused.columns else (\"vis_agitation\" if \"vis_agitation\" in df_fused.columns else \"agitation_score\")\n",
    "    if annotations_for_plot and globals().get('AUTO_ALIGN_GT', False):\n",
    "        annotations_for_plot, best_lag, best_corr = _align_gt(\n",
    "            annotations_for_plot,\n",
    "            df_fused,\n",
    "            score_col=score_col,\n",
    "            search_range=globals().get('LAG_SEARCH_SEC', (-5,5)),\n",
    "            steps=globals().get('LAG_STEPS', 41),\n",
    "        )\n",
    "        print(f\"üîß Auto-align GT: shift {best_lag:+.2f}s (corr={best_corr:.3f})\")\n",
    "    elif not annotations_for_plot:\n",
    "        print(\"‚ÑπÔ∏è No GT annotations: edit Step 2.\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è GT alignment disabled.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ===== –†–£–ß–ù–ê–Ø –†–ê–ó–ú–ï–¢–ö–ê (GROUND TRUTH) =====\n",
    "\n",
    "    label_colors = {\n",
    "        \"CALM\": \"rgba(0,200,100,0.15)\",\n",
    "        \"TENSE\": \"rgba(255,140,0,0.18)\",\n",
    "        \"ESCALATION\": \"rgba(200,0,0,0.18)\"\n",
    "    }\n",
    "\n",
    "    # ===== FIGURE: Reference-style (NO OVERLAP) =====\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # =========================\n",
    "    # 1. Ground Truth ‚Äî –æ—Ç–¥–µ–ª—å–Ω–∞—è –æ—Å—å Y2\n",
    "    # =========================\n",
    "    GT_COLORS = {\n",
    "        \"CALM\": \"rgba(46, 204, 113, 0.55)\",\n",
    "        \"TENSE\": \"rgba(243, 156, 18, 0.55)\",\n",
    "        \"ESCALATION\": \"rgba(231, 76, 60, 0.55)\",\n",
    "    }\n",
    "\n",
    "    for seg in annotations_for_plot:\n",
    "        label = seg[\"label\"].upper()\n",
    "\n",
    "        fig.add_shape(\n",
    "            type=\"rect\",\n",
    "            x0=seg[\"start\"],\n",
    "            x1=seg[\"end\"],\n",
    "            y0=0,\n",
    "            y1=1,\n",
    "            yref=\"y2\",\n",
    "            fillcolor=GT_COLORS[label],\n",
    "            line=dict(width=0),\n",
    "            layer=\"below\"\n",
    "        )\n",
    "\n",
    "        fig.add_annotation(\n",
    "            x=(seg[\"start\"] + seg[\"end\"]) / 2,\n",
    "            xref=\"x\",\n",
    "            y=0.99,\n",
    "            yref=\"paper\",\n",
    "            text=label,\n",
    "            showarrow=False,\n",
    "            font=dict(size=12, color=\"#2c3e50\"),\n",
    "        )\n",
    "\n",
    "    # =========================\n",
    "    # 2. Model ‚Äî –æ—Å–Ω–æ–≤–Ω–∞—è –æ—Å—å Y\n",
    "    # =========================\n",
    "\n",
    "    # Raw (—Ñ–æ–Ω)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_fused[\"time_sec\"],\n",
    "            y=df_fused[\"agitation_score\"],\n",
    "            name=\"Acoustic (raw)\",\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"rgba(120,120,120,0.3)\", width=1),\n",
    "            hoverinfo=\"skip\",\n",
    "            yaxis=\"y\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Smoothed (–æ—Å–Ω–æ–≤–Ω–∞—è –ª–∏–Ω–∏—è)\n",
    "    if \"vis_agitation\" in df_fused:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df_fused[\"time_sec\"],\n",
    "                y=df_fused[\"vis_agitation\"],\n",
    "                name=\"Acoustic (smoothed)\",\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=\"#E67E22\", width=3),\n",
    "                yaxis=\"y\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Fusion\n",
    "    if \"fusion_score\" in df_fused:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df_fused[\"time_sec\"],\n",
    "                y=df_fused[\"fusion_score\"],\n",
    "                name=\"Fusion (audio + text)\",\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=\"#C0392B\", width=2.5, dash=\"dash\"),\n",
    "                yaxis=\"y\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # =========================\n",
    "    # 3. Emotion zones (model)\n",
    "    # =========================\n",
    "    fig.add_hrect(\n",
    "        y0=0, y1=50,\n",
    "        fillcolor=\"rgba(46,204,113,0.08)\",\n",
    "        line_width=0,\n",
    "        layer=\"below\",\n",
    "        yref=\"y\"\n",
    "    )\n",
    "\n",
    "    fig.add_hrect(\n",
    "        y0=50, y1=100,\n",
    "        fillcolor=\"rgba(231,76,60,0.08)\",\n",
    "        line_width=0,\n",
    "        layer=\"below\",\n",
    "        yref=\"y\"\n",
    "    )\n",
    "\n",
    "    # =========================\n",
    "    # 4. Layout (–∫–ª—é—á–µ–≤–∞—è —á–∞—Å—Ç—å)\n",
    "    # =========================\n",
    "    fig.update_layout(\n",
    "        title=f\"Agitation vs Ground Truth ‚Äî {dropdown.value}\",\n",
    "        xaxis=dict(title=\"Time (sec)\"),\n",
    "\n",
    "        # –û—Å–Ω–æ–≤–Ω–∞—è –æ—Å—å ‚Äî –º–æ–¥–µ–ª—å\n",
    "        yaxis=dict(\n",
    "            title=\"Agitation score (0‚Äì100)\",\n",
    "            range=[0, 100],\n",
    "            domain=[0.0, 0.82]   # üëà –º–æ–¥–µ–ª—å —Å–Ω–∏–∑—É\n",
    "        ),\n",
    "\n",
    "        # –í—Ç–æ—Ä–∞—è –æ—Å—å ‚Äî GT\n",
    "        yaxis2=dict(\n",
    "            range=[0, 1],\n",
    "            domain=[0.86, 1.0],  # üëà GT —Å–≤–µ—Ä—Ö—É\n",
    "            visible=False\n",
    "        ),\n",
    "\n",
    "        hovermode=\"x unified\",\n",
    "        plot_bgcolor=\"white\",\n",
    "        paper_bgcolor=\"white\",\n",
    "        template=\"plotly_white\",\n",
    "        font=dict(size=13),\n",
    "\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.15,\n",
    "            xanchor=\"left\",\n",
    "            x=0\n",
    "        ),\n",
    "\n",
    "        margin=dict(l=70, r=20, t=90, b=50),\n",
    "        height=560\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aa2045d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìú –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —è–∑—ã–∫):\n",
      "\n",
      " My wife's family, they live in Gaza. They have cousins and uncles.  there and their house also was bombed. The question is  what is a proportionate response. It has been different from one tier to another. So if you look to this graph  for example, this is the death of Israeli and Palestinians, and it's changed from one year to a year.  It's like fluctuating like crypto. The lying son of a bitch lied to me. I told him you  don't understand. Ben Shapiro and Ron DeSantis keep saying that Israel warned you and Hamas asked you to  to keep to stay put. So I told you he's a loser. He never kept a job. He even like  and all of the interviews to become like a human shield, I would believe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_sec</th>\n",
       "      <th>end_sec</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>My wife's family, they live in Gaza. They hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>there and their house also was bombed. The qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.000</td>\n",
       "      <td>15.000</td>\n",
       "      <td>what is a proportionate response. It has been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>for example, this is the death of Israeli and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>It's like fluctuating like crypto. The lying ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start_sec  end_sec                                               text\n",
       "0      0.000    5.000   My wife's family, they live in Gaza. They hav...\n",
       "1      5.000   10.000   there and their house also was bombed. The qu...\n",
       "2     10.000   15.000   what is a proportionate response. It has been...\n",
       "3     15.000   20.000   for example, this is the death of Israeli and...\n",
       "4     20.000   25.000   It's like fluctuating like crypto. The lying ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 4 ‚Äî Full transcript (original language)\n",
    "if 'selected_path' not in globals() or not selected_path:\n",
    "    print(\"‚ö†Ô∏è Load audio in Step 1 first.\")\n",
    "elif not load_text_pipelines():\n",
    "    print(\"‚ö†Ô∏è Semantic models are not loaded.\")\n",
    "else:\n",
    "    df_text = transcribe_and_classify(selected_path, window_sec=5.0, hop_sec=5.0, sr=16000)\n",
    "    if df_text.empty:\n",
    "        print(\"‚ö†Ô∏è Could not get transcript.\")\n",
    "    else:\n",
    "        df_text = df_text.sort_values(\"start_sec\")\n",
    "        full_text = \" \".join(df_text[\"text\"].tolist())\n",
    "        print(\"üìú –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —è–∑—ã–∫):\\n\")\n",
    "        print(full_text)\n",
    "        display(df_text[[\"start_sec\", \"end_sec\", \"text\"]].head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion-venv",
   "language": "python",
   "name": "emotion-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
